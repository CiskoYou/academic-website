---
title: "Lab-3: Multiple linear regression"
author: "Youssouph Cissokho"
date: "`r Sys.time()`"
fontsize: 11pt 
header-inlcude:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
output:
  html_document:
    toc_depth:
    number_sections: true
  fig_caption: yes
urlcolor: blue
linkcolor: red
md_document:
    variant: markdown_strict+backtick_code_blocks+autolink_bare_uris
---

\definecolor{tcol}{HTML}{0000FF}

```{r setup, echo=FALSE}
library(formatR)
library(knitr)
options(width = 1000)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

`Table of content`

In this presentation, we will do the following 

* [Preliminaries](#11);
    a. [View structure of the dataset](#12);
    b. [Visualization of the dataset](#13);
* [Multiple linear regression](#1);
    a. [Multiple LR with full model](#2);
        i. [Significance of regression coefficients](#14)
        ii. [Significance of the model](#15)
    b. [LR with one predictor](#3) and [here](#4)
* [Model selection](#5)
    a. [Based on adjusted $R^2$, RSE and Mean$_{se}$](#6)
    b. [Based on AIC](#7)
    c. [Based on Anova](#8)
* [Confidence intervals for the parameters](#9)
* [CI on the mean response](#10)


<br>
<br>
<br>
<br>
<br>
<br>

# Preliminaries {#11}
A regression model that involves more than `one regressor variable` is called a multiple regression model. Fitting and analyzing these models is discussed in this lab. The results are extensions of those in `lab2` for simple linear regression.

The `multiple linear regression model with k` regressors (predictors) is given by 
$$
y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\ldots+\beta_{k} x_{k}+\varepsilon
$$
where 

* $\beta_i,\;\;i=1\cdots,k$ are the regression coefficients; 
* $\varepsilon$ is the random error i.e.
$$
\epsilon \sim N(0, \sigma^2).
$$
ie they are *independent and identically distributed* (iid) normal random variables with mean $0$ and variance $\sigma^2$.

`Goal` to estimate the regression coefficients    $\beta_i,\;\;i=1\cdots,k$.

## View structure of the dataset {#12}

We will use the `parenthood` datset from the textbook **Learning statistics with R** by `Danielle Navarro`. The question is to find out why Dan is so very grumpy (i.e. `bad-tempered and miserable`) the next day? Maybe, she is  not getting enough sleep. We drew some scatterplots to help us examine the relationship between the amount of sleep I get, and my grumpiness the following day

Now let's examine the `data` in details: The columns are 

* `baby.sleep`: the baby's sleep level (amount);
* `dan.grump` : danielle's grumpiness level the next day;
* `dan.sleep` : danielle's sleep level.

```{r data-exploration}
data_parent<-read.csv("parenthood.csv")   # load the parenthood data set 
head(data_parent, n=5)        # print the first 5 rows of the data
str(data_parent)              # display the internal structure of the               # elements of the dataset
dim(data_parent)              # gives the dimension 
```

## Visualization of the dataset {#13}

The `GGally` provides a function named `ggpairs` which is the ggplot2 equivalent of the pairs function in R.
```{r, message=FALSE}
# install.packages("GGally")
library(GGally)
ggpairs(data_parent, columns = 2:4) 
```

# Multiple linear regression {#1}

We will do 3 different regressions:

* Using all the predictors (`baby.sleep` and `dan.sleep`);
* Using one predictor at a time (`baby.sleep` or `dan.sleep`);
* Finally, we will choose the `best model.`

##  MLR with the full model {#2}

```{r }
# MLR with all the predictors
Full_model = lm(dan.grump ~ dan.sleep + baby.sleep, data = data_parent) 
Full_model     # print the coefficients
```
Hence, the multiple linear regression model is

y=`r round(Full_model$coefficients[1])` `r round(Full_model$coefficients[2])`* **dan.sleep** + `r round(Full_model$coefficients[3],3)`* **baby.sleep**

The coefficient associated with **dan.sleep** is quite large, suggesting that every `hour of sleep she loses makes her a lot grumpier`. However, the coefficient for **baby.sleep** is very small, suggesting that it doesn’t really matter how much sleep her son gets; not really. What matters as far as her grumpiness goes, is how much sleep she gets. 

Once the coefficients are estimated, we face two immediate questions:

1. `What is the overall adequacy of the model`? 
2. `Which specific regressors (coefficients) seem important`?

### The significance of regression coefficients {#14}

```{r }
# MLR with all the predictors
Full_model = lm(dan.grump ~ dan.sleep + baby.sleep, data = data_parent) 
summary(Full_model)     # print the coefficients
```

The third column gives you the t-statistic (each of the form $t=\frac{\hat{\beta}}{se(\hat{\beta})}$), and it’s actual p value for each of these tests (in the fourth column):

* The coef. intercept ($\hat{\beta}_0=125.97$) and the coef. of `dan.sleep` ($\hat{\beta}_1=-8.9$) are statistically significant i.e. p value ($p<0.001$).
* However, the coef. of `baby.sleep` ($\hat{\beta}_2=0.011$) is statistically not significant.

`Conclusion`: The `baby.sleep` variable has no significant effect; all the work is being done by the `dan.sleep` variable.

### Analysis-of-variance table and test for significance of regression of the model {#15}
```{r }
# MLR with all the predictors
anova(Full_model)
```
The F-test that is useful for checking the `performance of the model as a whole.`

In this case, the model performs significantly better since the F_stat is $215.2$ with a p value $p < .001$. Moreover, the adjusted $R^2=0.812$ indicates that the regression model accounts for $81.2\%$ of the variability in the outcome measure.

## Linear Regression with "dan.sleep" predictor {#3}


```{r }
# SLR with dan.sleep predictors
Dan_model = lm(dan.grump ~ dan.sleep, data = data_parent) 
summary(Dan_model)     # print the coefficients
anova(Dan_model)      # In SlR t and F tests are the same 
```
Hence, the simple linear regression model is

y=`r round(Full_model$coefficients[1])` `r round(Full_model$coefficients[2])`* **dan.sleep**

* The coef. intercept ($\hat{\beta}_0=125.97$) and the coef. of `dan.sleep` ($\hat{\beta}_1=-8.9$) are statistically significant i.e. p value ($p<0.001$).
* Moreover, the model performs significantly good, since the F_stat is $434.9$ with a p value $p < .001$. Moreover, the adjusted $R^2=0.814$ indicates that the regression model accounts for $81.4\%$ of the variability in the outcome measure.

## Linear Regression with "baby.sleep" predictor {#4}


```{r }
# SLR with baby.sleep predictors
Baby_model = lm(dan.grump ~ baby.sleep, data = data_parent) 
summary(Baby_model)     # print the coefficients
anova(Baby_model)      # In SlR t and F tests are the same 
```
Hence, the simple linear regression model is

y=`r round(Full_model$coefficients[1])` `r round(Full_model$coefficients[3],3)`* **baby.sleep**

* The coef. intercept ($\hat{\beta}_0=125.97$) and the coef. of `baby.sleep` ($\hat{\beta}_2=-2.74$) are statistically significant i.e. p value ($p<0.001$).
* Moreover, the **model performs significantly good**, since the F_stat is $46.2$ with a p value $p < .001$. Moreover, the adjusted $R^2=0.313$ indicates that the regression model accounts for $31.3\%$ of the variability in the outcome measure.

# Model accuracy assessment and Model selection {#5}
One fairly major problem that remains is the problem of`model selection`. That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of `variable selection`. Firstly, the two principles:

* Pick out a smallish number of possible predictors that are of theoretical interest;
* there is a trade off between simplicity and goodness of fit. You need to avoid throwing in too many variables.

Remember `Ockham’s razor` principle
>do not multiply entities beyond necessity. ie don’t >chuck in a bunch of largely irrelevant predictors just >to boost your

## Based on adjusted $R^2$, RSE and Mean$_{se}$ {#6}

The model accuracy can be assessed by examining the `adjusted R-squared` $R^2$, `Mean squared error` and `Residual Standard Error (RSE)`. 

We will summarize these values in this table.

| |Full model | Model 1 (with ban.sleep)| Model 2 (with baby.sleep)
|:---------|:------------:|:------------:|:------------:
|	$R^2_{adj}$| **81.2**$\%$| **81.4**$\%$| $31.3\%$
| RSE |	**4.354**| **4.332**| 8.327
| Mean$_{se}$ |**8159.9** |**8159.9**|3202.7 

**Note** a small Mean$_{se}$ is good.

Full model and model 1 (with `dan.sleep` as predictor) seem to have the same performance (higther $R^2_{adj}$ and lower `MSE`) and better than model 2 (with `baby.sleep` as a predictor). Now which one to choose? 
**IMPORTANT**, `Ockham’s razor` principle: the Simpler the better. Therefore, I assume Model 2 is simpler than Model 1 full model). That is 

y=`r round(Full_model$coefficients[1])` `r round(Full_model$coefficients[2])`* **dan.sleep**

## Based on the Akaike information criterion (AIC; Akaike, 1974) {#7}

The AIC for a model that has `K predictors` plus an intercept is:
$$
\mathrm{AIC}=\frac{\mathrm{SS}_{r e s}}{\hat{\sigma}^{2}}+2 K
$$
The `smaller the AIC value, the better the model performance is`
```{r }
AIC(Full_model,Dan_model)
```
Since the AIC of Model 1 (`dan.model`) is the lowest, then it's the best.

## based on Anova {#8}

tests the hypothesis that 

* $H_0$: model 1 (`dan_model`), selected predictors.
* $H_1$: full model ( all the predictors)
```{r }
anova(Dan_model,Full_model)
```
Since the p value $p=0.969 > 0.05$, hence, we fail to reject the null hypthesis i.e. `Model 1 is the winner`.

`NOTE:` This approach to regression, in which we add `all of our covariates` into a **null model**, and then add the `variables of interest` into an **alternative model**, and then compare the two models in hypothesis testing framework, is often referred to as `hierarchical regression`.

# Confidence intervals for the parameters {#9}

Recall that the $100(l − \alpha)$ percent confidence interval for the regression coefficient $\beta_j,\;j = 0, 1, \cdots ,k$, as
$$
\hat{\beta}_{j}-t_{\alpha / 2, n-p} \sqrt{\hat{\sigma}^{2} C_{j j}} \leq \beta_{j} \leq \hat{\beta}_{j}+t_{\alpha / 2, n-p} \sqrt{\hat{\sigma}^{2} C_{i j}}
$$
where,

$C_{jj}$ is the $j^{th}$ diagonal element of the $(X'X)^{−1}$ matrix. In R, a $95\%$ CIs for the parameters are obtained using **confint()**.

```{r }
confint(Full_model,level=0.95)
confint(Dan_model,level=0.95)
```
# 95% CI on the mean when danielle's sleep level is at 9.2 and 10.33 {#10}

Recall that, the $100(l-\alpha)$ percent confidence interval on the mean response at the point $\textbf{x}_0=(x_{01},x_{02},\cdots,x_{0k})'$ is

$$
\hat{y}_{0}-t_{\alpha / 2, n-p} \sqrt{\hat{\sigma}^{2} \mathbf{x}_{0}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{x}_{0}} \leq E\left(y \mid x_{0}\right) \leq \hat{y}_{0}+t_{\alpha / 2, n-p} \sqrt{\hat{\sigma}^{2} \mathbf{x}_{0}^{\prime}\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{x}_{0}}
$$
where,

* $\textbf{x}_0=(x_{01},x_{02},\cdots,x_{0k})$ are feature values;
* $(X'X)^{−1}$ is the covariate matrix. In R, we use `predict` function with `interval="confidence:"`

```{r }
new <- data.frame(dan.sleep = c(9.2,10.33))  # create a new data

# this provides a 95% confidence interval on the mean response
predict(Dan_model, new, interval="confidence")    
```
Hence, 

* the CI of the mean response for `dan.sleep=9.2` is $41.74\leq E\left(y \mid x_{0}\right) \leq 45.82$;
* the CI of the mean response for `dan.sleep=10.33` is $30.64\leq E\left(y \mid x_{0}\right) \leq 36.63$.


# Summary

In this presentation, we discussed about 

* [Multiple linear regression](#1)
    a. [Multiple LR with full model](#2)
    c. [LR with one predictor](#3) and [here](#4)
* [Model selection](#5)
    a. [Based on adjusted $R^2$, RSE and Mean$_{se}$](#6)
    b. [Based on AIC](#7)
    c. [Based on Anova](#8)
* [Confidence intervals for the parameters](#9)
* [CI on the mean response](#10)
  

# Problem to do (p3.8 page 123)

The data in Table B.5 present the performance of a chemical process as a function of sever controllable process variables.

a. Fit a multiple regression model relating $CO_2$ product (y) to total solvent ($x_6$) and hydrogen consumption ($x_7$)
b. Test for significance of regression. Calculate $R^2$ and $R^2_{Adj}$.
c. Using t tests determine the contribution of $x_6$ and $x_7$ to the model.
d. Construct $95\%$ CIs on $\beta_6$ and $\beta_7$.
e. Refit the model using only $x_6$ as the regressor. Test for significance of regression and calculate $R^2$ and $R^2_{Adj}$. Discuss your findings. Based on these statistics, are you satisfied with this model?
f. Construct a $95\%$ CI on $\beta_6$ using the model you fit in part e. Compare the length of this CI to the length of the CI in part d. Does this tell you anything important about the contribution of $x_7$ to the model?
g. Compare the 2 models

